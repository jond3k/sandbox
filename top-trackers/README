This is a mashup of Ghostery and Alexa which tries to determine the most popular
tracking scripts on the web. I thoroughly recommend Ghostery by the way, it's a
really good browser plugin.

At the time of writing, these are the top 20 people who know where you are: 

1  6933 Twitter Button
2  4379 Quantcast
3  3499 Google AdWords Conversion
4  3393 OpenX
5  3101 LiveInternet
6  3084 ScoreCard Research Beacon
7  2499 CNZZ
8  2058 Twitter Badge
9  1786 Statcounter
10 1703 Whos.amung.us
11 1661 ChartBeat
12 1643 Google Custom Search Engine
13 1551 Wordpress Stats
14 1517 ShareThis
15 1326 Histats
16 1073 Crazy Egg
17 1041 BuySellAds
18 1025 Alexa Traffic Rank
19 970  DoubleClick Floodlight
20 925 Gemius

=== Contents ===

ghostery.json                   Ghostery's data file. A json object containing
                                the and names of 'bugs' (trackers, etc) and
                                patterns used to match them
top-100k-collate-all.sh         Collate the log file and get the top trackers
top-100k-collate-trackers.sh    Collate the log file and get the top trackers
                                where ghostery bug type=tracker
top-100k.csv                    A head -n100000 sample of the Alexa top
                                1,000,000 list generated on 8th Jan 2012
top-100k.err                    The error log generated by top-bugs-many.sh
top-100k.log                    The trackers found by top-bugs-many.sh
top-trackers-many.sh            Kick starts a bunch of top-bugs.php scripts,
                                each with a certain range
top-trackers-multi.php          An experiment to make parallelism easy with curl
                                multi. Didn't work too well. Don't use PHP for
                                things that aren't single threaded :-)
top-trackers.php                Iterates a range of sites in top-100k.csv, scans
                                them for trackers and writes the name of each
                                one encountered to stdout

=== Summary ===

I have retrieved Ghostery's data file and matched its patterns against the index
page of the top 100,000 Alexa pages. I found the Ghostery data file by analysing
network traffic between the plugin and site and the Alexa dataset was a free
downloadable.

The mashup is powered by a PHP script (not my favourite language
but I've spent a lot of time with it recently) which I then parallelised through
multiple simultaneous runs trigged by a bash script. Each PHP process iterates
over a chunk of the Alexa site list and emits the names of any trackers found to
stdout. This output can then be collated using standard linux command line
tools. Connection errors and performance measurements are sent to stderr.

The resulting mashup is open for interpretation. For example, should we assume
all scripts discovered are potential trackers, or should we filter specifically,
trusting Ghostery's type categorization (trackers, ads, widgets and so on)? Here
are the top 10 of all matches and their frequency:

  > bash top-100k-collate-all.sh | head -n10
   6933 Twitter Button
   4379 Quantcast
   3499 Google AdWords Conversion
   3393 OpenX
   3101 LiveInternet
   3084 ScoreCard Research Beacon
   2499 CNZZ
   2058 Twitter Badge
   1786 Statcounter
   1703 Whos.amung.us

Some might dispute the categorisation of the retweet button as a tracker. We can
instead use Ghostery's tracker classification (type=tracker in the json file) to
get the following top 10:

  > bash top-100k-collate-trackers.sh | head -n10
   3084 ScoreCard Research Beacon
   1551 Wordpress Stats
    970 DoubleClick Floodlight
    910 WebTrends
    841 Tynt Insight
    537 Audience Science
    487 Rambler
    483 StumbleUpon Widgets
    285 Optimizely
    281 Visual Website Optimizer

You can do your own investigation by collating the top-100k.log with standard
linux commands:

# get the top 10 matches
sort top-100k.log | uniq -c | sort -nr | head -n10

=== Issues ===

Depth of search: The script only checks the index page of a site. Trackers might
only be active on deeper pages. A deeper inspection of the site graph might
increase the number of trackers found.

Failed requests: The script will report an error for failed requests but will
not reattempt. Failure reasons include connection timeout, transfer timeout, too
many redirects, failed to resolve host. Despite this, the log file can be
grepped to find the sites that failed.

    > grep "request took" top-100k.err | wc -l
    72439  

    > grep "Failed to get content" top-100k.err | wc -l
    5583

Lots of manual intervention: The script needs manual processing of output and
errors in order to generate results and determine which ones needs to be run
again

Scalability: It took 9 hours to process less than 80,000 sites. This script is
clearly not making the best of network and cpu resources. Creating multiple PHP
scripts is an ugly approach. If I needed to rerun this test on a regular basis
there might be some value in building 

=== Ghostery ===

To find out what Ghostery is doing when we press the Update button, run this
command:

  > sudo ngrep -d wlan0 port 80 | yay host is red

We get this:

T 192.168.0.13:51147 -> 107.21.249.146:80 [AP]
  GET /update/bugs?format=json&_=1326025878458 HTTP/1.1..Host: www.ghostery.com..Connection: keep-alive..User-Agen
  t: Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.75 Safari/535.7..Accept: 
  application/json, text/javascript, */*; q=0.01..Accept-Encoding: gzip,deflate,sdch..Accept-Language: en-GB,en-US
  ;q=0.8,en;q=0.6..Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3....

e.g. We can wget http://www.ghostery.com/update/bugs?format=json&_=1326025878458
to get a full bug list to use in our script

Ghostery data looks like this:

jazor ghostery.json | head -n12
{
  "bugsVersion": "65",
  "bugs": [
    {
      "name": "IndexTools",
      "id": "5",
      "pattern": "indextools\\.js",
      "aid": "1",
      "type": "analytics",
      "cid": "451",
      "affiliation": "IAB,DMA,NAI,TRUSTe,DAA,AAF,IAB Europe"
    },

jazor ghostery.json | grep \"pattern\" | head -n5
      "pattern": "indextools\\.js",
      "pattern": "static\\.scribefire\\.com\\/ads\\.js",
      "pattern": "(static\\.getclicky\\.com\\/|hello\\.staticstuff\\.net\\/|clicky\\.js)",
      "pattern": "statisfy\\.net\\/javascripts\\/stats\\.js",
      "pattern": "gmodules\\.com\\/",

We need to match every script url against bugs[*].pattern

"ReTargeter Beacon" is automatically removed because it has an invalid
regular expression

=== Amazon ===

Options:
- There's an aws alexa api but it would cost $250 for 100,000 urls
  http://aws.amazon.com/alexatopsites/#pricing
- Stack Overflow says: we can use the alexa toolbar api to get information PER
  domain. damn http://data.alexa.com/data?cli=10&dat=snbamz&url=%YOUR_URL%
- Scraping alexa, but there's only 500 domains listed on the main page
- Looking for a torrent

..but then I  noticed a free download with 1,000,000 entries updated today.
woohoo: wget http://s3.amazonaws.com/alexa-static/top-1m.csv.zip

The dataset's layout is a CSV

head -n5 top-1m.csv
1,google.com
2,facebook.com
3,youtube.com
4,yahoo.com
5,baidu.com

Let's keep it small: 
  > head -n 100000 top-1m.csv > top-100k.csv

We build a map of links[colN] = domainN and curl over them




